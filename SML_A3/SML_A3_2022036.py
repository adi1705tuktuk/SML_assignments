# -*- coding: utf-8 -*-
"""SML_A3_2022036.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PTUo3s-NO6oD2tXCvSIqhmUSjnENU8Rw
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.utils import resample
from sklearn.tree import DecisionTreeClassifier

import matplotlib.pyplot as plt

# Load the MNIST dataset
data = np.load('/content/drive/MyDrive/SML_A3/mnist.npz')
x_train = data['x_train']
y_train = data['y_train']
x_test = data['x_test']
y_test = data['y_test']

# Select classes 0, 1, and 2
train_mask = np.isin(y_train, [0, 1, 2])
x_train_subset = x_train[train_mask]
y_train_subset = y_train[train_mask]

test_mask = np.isin(y_test, [0, 1, 2])
x_test_subset = x_test[test_mask]
y_test_subset = y_test[test_mask]

# Reshape the data to 2D
x_train_subset = x_train_subset.reshape(x_train_subset.shape[0], -1)
x_test_subset = x_test_subset.reshape(x_test_subset.shape[0], -1)

# Perform PCA on the training data
pca = PCA(n_components=10)
x_train_pca = pca.fit_transform(x_train_subset)
x_test_pca = pca.transform(x_test_subset)

# Print the explained variance ratio
print("Explained variance ratio:", sum(pca.explained_variance_ratio_))

# Plot the explained variance ratio
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Cumulative Explained Variance Ratio by Principal Components')
plt.grid(True)
plt.show()

# Function to calculate Gini index
def gini_index(labels):
    _, counts = np.unique(labels, return_counts=True)
    p = counts / len(labels)
    return 1 - np.sum(p**2)

# Function to find the best split for a node
def find_best_split(x, y):
    best_gini = float('inf')
    best_split = None
    best_threshold = None

    for feature in range(x.shape[1]):
        sorted_indices = np.argsort(x[:, feature])
        sorted_x = x[sorted_indices]
        sorted_y = y[sorted_indices]

        for i in range(len(sorted_x) - 1):
            if sorted_y[i] != sorted_y[i + 1]:
                threshold = (sorted_x[i, feature] + sorted_x[i + 1, feature]) / 2
                left_indices = sorted_indices[:i + 1]
                right_indices = sorted_indices[i + 1:]

                left_gini = gini_index(sorted_y[left_indices])
                right_gini = gini_index(sorted_y[right_indices])
                weighted_gini = (len(left_indices) / len(sorted_y)) * left_gini + (len(right_indices) / len(sorted_y)) * right_gini

                if weighted_gini < best_gini:
                    best_gini = weighted_gini
                    best_split = (feature, threshold, left_indices, right_indices)

    return best_split

# Function to grow the decision tree with 3 terminal nodes
def grow_tree(x, y):
    root_split = find_best_split(x, y)
    root_feature, root_threshold, root_left_indices, root_right_indices = root_split

    left_child_split = find_best_split(x[root_left_indices], y[root_left_indices])
    right_child_split = find_best_split(x[root_right_indices], y[root_right_indices])

    return root_split, left_child_split, right_child_split

# Function to plot decision tree splits
def plot_decision_tree_splits(root_split, left_child_split, right_child_split, x_train_pca, y_train_subset):
    root_feature, root_threshold, _, _ = root_split
    left_feature, left_threshold, _, _ = left_child_split
    right_feature, right_threshold, _, _ = right_child_split

    plt.figure(figsize=(12, 6))

    for label, c, marker in zip([0, 1, 2], ['blue', 'green', 'red'], ['o', 'x', 's']):
        mask = y_train_subset == label
        plt.scatter(x_train_pca[mask, root_feature], y_train_subset[mask], c=c, marker=marker, label=f'Class {label}')

    plt.axvline(x=root_threshold, color='black', linestyle='--', label='Root Threshold')
    plt.axvline(x=left_threshold, color='green', linestyle='--', label='Left Child Threshold')
    plt.axvline(x=right_threshold, color='red', linestyle='--', label='Right Child Threshold')

    plt.xlabel('Principal Component Feature')
    plt.ylabel('Class Label')
    plt.title('Decision Tree Splits')
    plt.legend()
    plt.grid(True)
    plt.show()

# Grow the decision tree with 3 terminal nodes
root_split, left_child_split, right_child_split = grow_tree(x_train_pca, y_train_subset)

# Plot the decision tree splits
plot_decision_tree_splits(root_split, left_child_split, right_child_split, x_train_pca, y_train_subset)

# Make predictions on the test data
y_pred = np.zeros(len(y_test_subset), dtype=int)

root_feature, root_threshold, root_left_indices, root_right_indices = root_split
left_feature, left_threshold, left_left_indices, left_right_indices = left_child_split
right_feature, right_threshold, right_left_indices, right_right_indices = right_child_split

for i in range(len(y_test_subset)):
    if x_test_pca[i, root_feature] < root_threshold:
        if x_test_pca[i, left_feature] < left_threshold:
            y_pred[i] = np.bincount(y_train_subset[root_left_indices[left_left_indices]]).argmax()
        else:
            y_pred[i] = np.bincount(y_train_subset[root_left_indices[left_right_indices]]).argmax()
    else:
        if x_test_pca[i, right_feature] < right_threshold:
            y_pred[i] = np.bincount(y_train_subset[root_right_indices[right_left_indices]]).argmax()
        else:
            y_pred[i] = np.bincount(y_train_subset[root_right_indices[right_right_indices]]).argmax()

# Calculate accuracy
accuracy = accuracy_score(y_test_subset, y_pred)
print("Accuracy:", accuracy)

# Function to predict the class of a sample
def predict_class(sample, root_split, left_child_split, right_child_split):
    root_feature, root_threshold, root_left_indices, root_right_indices = root_split
    left_feature, left_threshold, left_left_indices, left_right_indices = left_child_split
    right_feature, right_threshold, right_left_indices, right_right_indices = right_child_split

    if sample[root_feature] < root_threshold:
        if sample[left_feature] < left_threshold:
            return np.bincount(y_train_subset[root_left_indices[left_left_indices]]).argmax()
        else:
            return np.bincount(y_train_subset[root_left_indices[left_right_indices]]).argmax()
    else:
        if sample[right_feature] < right_threshold:
            return np.bincount(y_train_subset[root_right_indices[right_left_indices]]).argmax()
        else:
            return np.bincount(y_train_subset[root_right_indices[right_right_indices]]).argmax()

# Make predictions on the test data
y_pred = np.array([predict_class(x_test_pca[i], root_split, left_child_split, right_child_split) for i in range(len(y_test_subset))])

# Make predictions on the test data
y_pred = np.array([predict_class(x_test_pca[i], root_split, left_child_split, right_child_split) for i in range(len(y_test_subset))])

# Calculate overall accuracy
accuracy = accuracy_score(y_test_subset, y_pred)
print("Overall Accuracy:", accuracy)

# Calculate class-wise accuracy
conf_matrix = confusion_matrix(y_test_subset, y_pred)
class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)
print("Class-wise Accuracy:")
for i, acc in enumerate([0, 1, 2]):
    print(f"Class {acc}: {class_accuracies[i]}")

# Plot class-wise accuracy
plt.bar([0, 1, 2], class_accuracies)
plt.xlabel('Class')
plt.ylabel('Accuracy')
plt.title('Class-wise Accuracy')
plt.xticks([0, 1, 2], [0, 1, 2])
plt.ylim(0, 1)
plt.grid(axis='y')
plt.show()

# Function to bootstrap a dataset
def bootstrap_dataset(x, y):
    indices = np.random.choice(len(x), len(x), replace=True)
    return x[indices], y[indices]

# Build decision trees using bagging
trees = []
for _ in range(5):
    x_bootstrap, y_bootstrap = bootstrap_dataset(x_train_pca, y_train_subset)
    tree = DecisionTreeClassifier(max_leaf_nodes=3)
    tree.fit(x_bootstrap, y_bootstrap)
    trees.append(tree)

# Make predictions using majority voting
y_pred = []
for sample in x_test_pca:
    votes = [tree.predict([sample])[0] for tree in trees]
    counts = np.bincount(votes)
    if np.max(counts) >= 3:
        y_pred.append(np.argmax(counts))
    else:
        y_pred.append(votes[0])  # Choose either class in case of a tie

y_pred = np.array(y_pred)

# Calculate overall accuracy
accuracy = accuracy_score(y_test_subset, y_pred)
print("Overall Accuracy:", accuracy)

# Calculate class-wise accuracy
conf_matrix = confusion_matrix(y_test_subset, y_pred)
class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)
print("Class-wise Accuracy:")
for i, acc in enumerate([0, 1, 2]):
    print(f"Class {acc}: {class_accuracies[i]}")

# Plot class-wise accuracy
plt.bar([0, 1, 2], class_accuracies)
plt.xlabel('Class')
plt.ylabel('Accuracy')
plt.title('Class-wise Accuracy')
plt.xticks([0, 1, 2], [0, 1, 2])
plt.ylim(0, 1)
plt.grid(axis='y')
plt.show()

